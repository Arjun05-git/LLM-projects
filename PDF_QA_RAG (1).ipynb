{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIYacPFoV8HD"
   },
   "source": [
    "# Retrieval Augmented Generation (RAG) Pipeline for PDF Question Answering \n",
    "\n",
    "## Overview\n",
    "\n",
    "This project implements a Retrieval Augmented Generation (RAG) pipeline to answer questions based on the content of PDF documents.  It combines information retrieval with a large language model (LLM) GPT-2 here to provide more accurate and contextually relevant answers. The project utilizes several Python libraries, including PyMuPDF, pdfminer.six, sentence-transformers, FAISS, and transformers.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The project operates on a collection of PDF documents.The documents were created through prompting and are documents titled Introduction to Machine Learning,Basics of NLP and Introduction to Data Sciece.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Data Acquisition and Preparation:**\n",
    "    - PDF files are read from a specified directory.\n",
    "    - Text is extracted from each PDF using `PyMuPDF` and `pdfminer.six`.\n",
    "    - The extracted text is chunked into smaller, overlapping segments to manage context length and improve retrieval granularity(Sliding Window Chunking).\n",
    "\n",
    "2. **Embedding Generation:**\n",
    "    - The `sentence-transformers` library is used to generate embeddings for each text chunk.  The `all-MiniLM-L6-v2` model is used for embedding generation.\n",
    "    - These embeddings are vector representations of the text, capturing semantic meaning.\n",
    "\n",
    "3. **FAISS Indexing:**\n",
    "    - A FAISS index (`IndexFlatL2`) is created to store and efficiently search the generated embeddings.  This allows for fast retrieval of relevant chunks given a query.\n",
    "\n",
    "4. **Retrieval:**\n",
    "    - When a user provides a query, its embedding is generated using the same `sentence-transformers` model.\n",
    "    - The FAISS index is queried to find the most similar (and therefore most relevant) text chunks based on the query embedding.\n",
    "\n",
    "5. **Language Model Interaction and Response Generation:**\n",
    "    - A pre-trained language model (`gpt2-large`) from the `transformers` library is loaded.\n",
    "    - A prompt is constructed containing the retrieved relevant chunks and the user's query.  The context is truncated to fit within the LLM's context window.\n",
    "    - The LLM generates a response based on the provided prompt.\n",
    "\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "The project provides a functional RAG pipeline.  The quality of the responses depends on factors like the quality of the PDFs, the chunking strategy, the choice of embedding model, and the LLM used.  The `gpt2-large` model is used in this example.The model understood which PDF document was related to the query and provided an adequete reasponse.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project demonstrates a basic RAG pipeline for PDF question answering. It provides a foundation for building more sophisticated systems. Future work will focus on improving context handling, optimizing GPU utilization, enhancing prompt engineering, and implementing evaluation metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBu8wAo_zCMq",
    "outputId": "daf40349-7c0b-4689-c1ce-23f18937f6b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.25.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Downloading pymupdf-1.25.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF, faiss-cpu, pdfminer.six\n",
      "Successfully installed PyMuPDF-1.25.0 faiss-cpu-1.9.0.post1 pdfminer.six-20240706\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF pdfminer.six torch transformers sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54JO6s_bvSjf",
    "outputId": "3b31549c-c2d4-45e5-b772-559a0066a161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: conda: command not found\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision cudatoolkit=11.1 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exoddwEf07EM",
    "outputId": "77d03de6-3d25-49fc-b540-6f969b9afcd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "#necessary libraries\n",
    "import os\n",
    "import io\n",
    "import fitz\n",
    "from pdfminer.high_level import extract_text\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hscCflZA0_Fx",
    "outputId": "13179d7f-ebc6-4bea-db73-9a7522e96830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#checking cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGIDIhih3SHw",
    "outputId": "e8118865-3094-4265-8c76-6c82c66c2294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PDF files from: /content/sample_data/pdfs\n"
     ]
    }
   ],
   "source": [
    "#pdfs created and added to this path\n",
    "pdf_folder = '/content/sample_data/pdfs'\n",
    "print(f\"Using PDF files from: {pdf_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnx_EmNksjSR",
    "outputId": "9cada24e-0652-43cd-bd53-268709fd7069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3 PDF files.\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "documents = []\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(pdf_folder, filename)\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        documents.append((filename, text))\n",
    "\n",
    "print(f\"Processed {len(documents)} PDF files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1Zert-csjYQ",
    "outputId": "bbc4a1aa-9334-4e9a-8c28-28d215c9bf4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 chunks.\n"
     ]
    }
   ],
   "source": [
    "#Chunking using sliding window chunking\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "chunked_documents = []\n",
    "for filename, text in documents:\n",
    "    chunks = chunk_text(text)\n",
    "    chunked_documents.extend([(filename, chunk) for chunk in chunks])\n",
    "\n",
    "print(f\"Created {len(chunked_documents)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1kb0Tg0vqoe",
    "outputId": "37f46187-1118-473a-cb3b-8ab61496188b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 39.56427001953125 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"GPU Memory:\", torch.cuda.get_device_properties(0).total_memory / (1024.0 **3), \"GB\")\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "qArYpnlnsjcQ",
    "outputId": "cd57cca0-1a48-4fbf-d952-b2733e7dd3ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-3487c572dff4>:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  embeddings = torch.tensor(embeddings)\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing the chunks\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)\n",
    "else:\n",
    "    model.to('cpu')\n",
    "\n",
    "batch_size = 32\n",
    "embeddings = []\n",
    "for i in range(0, len(chunked_documents), batch_size):\n",
    "    batch = chunked_documents[i:i+batch_size]\n",
    "    batch_embeddings = model.encode([chunk for _, chunk in batch], convert_to_tensor=True)\n",
    "    if torch.cuda.is_available():\n",
    "        batch_embeddings = batch_embeddings.cpu().numpy()\n",
    "    else:\n",
    "        batch_embeddings = batch_embeddings.numpy()\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "embeddings = torch.tensor(embeddings)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7AHHMf-cswgH",
    "outputId": "20bc8a47-890e-4a33-c2a5-04c5260e78ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created.\n"
     ]
    }
   ],
   "source": [
    "#Indexing using FAISS for easy storage and search of embeddings\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings.numpy())\n",
    "\n",
    "print(\"FAISS index created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o12t4quYswRf"
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, top_k=5):\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    return [chunked_documents[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "f4lxAL9fswNh",
    "outputId": "a6cddb7c-7490-4d32-99da-af37b465a8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model loaded.\n"
     ]
    }
   ],
   "source": [
    "#Loading gpt-2 using transformers\n",
    "model_name = \"gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(\"Language model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8MMl76Ms2rB"
   },
   "outputs": [],
   "source": [
    "# function that generates response based on query based on these specified hyperparameters\n",
    "def generate_response(query, max_length=200):\n",
    "    relevant_chunks = retrieve_relevant_chunks(query)\n",
    "    context = \"\\n\".join([chunk for _, chunk in relevant_chunks])\n",
    "\n",
    "    context = context[:1024 - len(query) - 20]\n",
    "\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = lm_model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=min(1024, len(input_ids[0]) + max_length),\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVi06lEss6Z_",
    "outputId": "372b6bbc-bb1c-46b1-e50f-a20d1d0ba393"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What algorithms are mentioned in the PDF 'Introduction to Machine Learning' for each type of machine learning?\n",
      "Response: Context: Introduction to Machine Learning \n",
      "\n",
      "Machine learning is a rapidly growing field of artificial intelligence that focuses on the development \n",
      "of algorithms and statistical models that enable computer systems to improve their performance on \n",
      "a specific task through experience. At its core, machine learning is about creating systems that can \n",
      "learn from data, identify patterns, and make decisions with minimal human intervention. This \n",
      "approach has revolutionized numerous industries, from healthcare and finance to transportation and \n",
      "entertainment.There are three main types of machine learning: supervised learning, unsupervised \n",
      "learning, and reinforcement learning. Supervised learning involves training models on labeled data to \n",
      "make predictions or classifications. Unsupervised learning, on the other hand, deals with finding \n",
      "hidden patterns or structures in unlabeled data. Reinforcemen\n",
      "\n",
      "Question: What algorithms are mentioned in the PDF 'Introduction to Machine Learning' for each type of machine learning?\n",
      "\n",
      "Answer: The introduction to Machine Learning contains a brief description of each of these algorithms.\n",
      "\n",
      "Supervised Learning\n",
      "\n",
      "Supervised Learning is the first type of machine learning. This type of machine learning is based on the use of supervised \n",
      "\n",
      "experiments or supervised learning algorithms. The most common example of supervised learning is linear regression.\n",
      "\n",
      "Linear regression is a form of regression that is based on a linear relationship between two variables. In this case, the variable\n",
      "\n",
      "is a predictor (a predictor is a variable that is used to predict another variable). The variable is the variable being\n",
      "\n",
      "predicted, and the relationship between the predictor and the predictor is the relationship between the variable and its value.\n",
      "\n",
      "The linear relationship between the predictor and the variable is called a regression. In this example, the variable \"student\" is the\n",
      "\n",
      "predicted value of the student. The relationship between student and its value is called the predictor.\n",
      "\n",
      "The linear relationship between the predictor and the\n"
     ]
    }
   ],
   "source": [
    "#The query\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "query = \"What algorithms are mentioned in the PDF 'Introduction to Machine Learning' for each type of machine learning?\"\n",
    "response = generate_response(query)\n",
    "print(\"Query:\", query)\n",
    "print(\"Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
